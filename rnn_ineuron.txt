1. What are deep learning modelling techniques to handle textual data?
* CBOW model in which the distributed representations of context (or surrounding words) are combined to predict the word in the middle.
* Skip-gram modelin which the distributed representation of the input word is used to predict the context .




2. What is RNN?Why we use them?
It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for
problems that involve sequential data .Recurrent Neural Network is a generalization of feedforward neural network 
that has an internal memory. RNN is recurrent in nature as it performs the same function for every input of data while
the output of the current input depends on the past one computation. After producing the output, it is copied and sent 
back into the recurrent network. For making a decision, it considers the current input and the output that it has learned
from the previous input. It is used mainly because its capability to handle sequential data .

3. State down few problems in RNN?
  * Gradient vanishing and exploding problems
  * Training an RNN is a very difficult task
  * It cannot process long sequences


4. What is vanishing gradient and gradient explosion?
 In a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large then the gradient will
 increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of 
 exploding gradient. Alternatively, if the derivatives are small then the gradient will decrease exponentially as we propagate
 through the model until it eventually vanishes, and this is the vanishing gradient problem .

5.What is LSTM ?
 LSTM stands for long short term memory. It is a model or architecture that extends the memory of recurrent neural networks.
 Typically, recurrent neural networks have ‘short term memory’ in that they use persistent previous information to be used in 
 the current neural network. Essentially, the previous information is used in the present task. That means we do not have a list 
 of all of the previous information available for the neural node.It has Memory Gate , Input Gate and Output Gate.


6.Benifits of LSTM over RNN?
We can say that, when we move from RNN to LSTM, we are introducing more & more controlling knobs, which control the flow and mixing 
of Inputs as per trained Weights. And thus, bringing in more flexibility in controlling the outputs. So, LSTM gives us the most 
Control-ability and thus, Better Results.
It is better than rnn to consider the long term dependencies
